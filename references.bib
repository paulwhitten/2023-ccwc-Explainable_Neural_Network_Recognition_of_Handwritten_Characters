@article{McCulloch1943-MCCALC-5,
	author = {Warren S. McCulloch and Walter Pitts},
	volume = {5},
	journal = {The Bulletin of Mathematical Biophysics},
	number = {4},
	doi = {10.1007/bf02478259},
	title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
	year = {1943},
	pages = {115--133}
}

@book{rosenblatt1957perceptron,
  title={The Perceptron, a Perceiving and Recognizing Automaton Project Para},
  author={Rosenblatt, F.},
  series={Report: Cornell Aeronautical Laboratory},
  url={https://books.google.com/books?id=P\_XGPgAACAAJ},
  year={1957},
  publisher={Cornell Aeronautical Laboratory}
}

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

  @ARTICLE{6795724,
  author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
  doi={10.1162/neco.1989.1.4.541}}

  @ARTICLE{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}

@INPROCEEDINGS{whitten21,
  author={Whitten, Paul and Wolff, Francis and Papachristou, Chris},
  booktitle={NAECON 2021 - IEEE National Aerospace and Electronics Conference}, 
  title={Explainable Artificial Intelligence Methodology for Handwritten Applications}, 
  year={2021},
  volume={},
  number={},
  pages={277-282},
  doi={10.1109/NAECON49338.2021.9696413},
  url={https://doi.org/10.1109/NAECON49338.2021.9696413}
  }

  @inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{angelova05,
  author={Angelova, A. and Abu-Mostafam, Y. and Perona, P.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Pruning training sets for learning of object categories}, 
  year={2005},
  volume={1},
  number={},
  pages={494-501 vol. 1},
  doi={10.1109/CVPR.2005.283}}

@ARTICLE{keysers07,
  author={Keysers, Daniel and Deselaers, Thomas and Gollan, Christian and Ney, Hermann},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Deformation Models for Image Recognition}, 
  year={2007},
  volume={29},
  number={8},
  pages={1422-1435},
  doi={10.1109/TPAMI.2007.1153}}

  @article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{Gunning_Aha_2019,
  title={DARPA’s Explainable Artificial Intelligence (XAI) Program},
  volume={40},
  url={https://ojs.aaai.org/index.php/aimagazine/article/view/2850},
  DOI={10.1609/aimag.v40i2.2850},
  number={2},
  journal={AI Magazine},
  author={Gunning, David and Aha, David},
  year={2019},
  month={Jun.},
  pages={44-58}
  }


@article{northcutt2021,
author = {Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
title = {Confident Learning: Estimating Uncertainty in Dataset Labels},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12125},
doi = {10.1613/jair.1.12125},
abstract = {Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {1373–1411},
numpages = {39}
}

@article{fukushima1982neocognitron,
  title={Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position},
  author={Fukushima, Kunihiko and Miyake, Sei},
  journal={Pattern recognition},
  volume={15},
  number={6},
  pages={455--469},
  year={1982},
  publisher={Elsevier}
}

@article{FUKUSHIMA2003161,
title = {Neocognitron for handwritten digit recognition},
journal = {Neurocomputing},
volume = {51},
pages = {161-180},
year = {2003},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(02)00614-8},
url = {https://www.sciencedirect.com/science/article/pii/S0925231202006148},
author = {Kunihiko Fukushima},
keywords = {Visual pattern recognition, Neural network model, Multi-layered network, Neocognitron, Handwritten digit},
abstract = {The author previously proposed a neural network model neocognitron for robust visual pattern recognition. This paper proposes an improved version of the neocognitron and demonstrates its ability using a large database of handwritten digits (ETL1). To improve the recognition rate of the neocognitron, several modifications have been applied: such as, the inhibitory surround in the connections from S-cells to C-cells, contrast-extracting layer between input and edge-extracting layers, self-organization of line-extracting cells, supervised competitive learning at the highest stage, staggered arrangement of S- and C-cells, and so on. These modifications allowed the removal of accessory circuits that were appended to the previous versions, resulting in an improvement of recognition rate as well as simplification of the network architecture. The recognition rate varies depending on the number of training patterns. When we used 3000 digits (300 patterns for each digit) for the learning, for example, the recognition rate was 98.6% for a blind test set (3000 digits), and 100% for the training set.}
}

@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}

@article{vilone2020explainable,
  title={Explainable artificial intelligence: a systematic review},
  author={Vilone, Giulia and Longo, Luca},
  journal={arXiv preprint arXiv:2006.00093},
  year={2020}
}

@article{Hough1959qva,
    author = "Hough, P. V. C.",
    editor = "Kowarski, L.",
    title = "{Machine Analysis of Bubble Chamber Pictures}",
    journal = "Conf. Proc. C",
    volume = "590914",
    pages = "554--558",
    year = "1959"
}

@article{Arrieta2020ExplainableAI,
  title={Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  author={A. Arrieta and Natalia D'iaz-Rodr'iguez and J. Ser and Adrien Bennetot and S. Tabik and A. Barbado and Salvador Garc'ia and Sergio Gil-L'opez and D. Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.10045}
}

@INPROCEEDINGS{Survey18, 
author={F. K. {Došilović} and M. {Brčić} and N. {Hlupić}}, 
booktitle={2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, 
title={Explainable artificial intelligence: A survey}, 
year={2018}, 
volume={}, 
number={}, 
pages={0210-0215}, 
keywords={learning (artificial intelligence);interpretability;healthcare;finance;explainable artificial intelligence;XAI;recent developments;supervised learning;artificial general intelligence;datasets;computing power;machine learning systems;(super)human performance;image recognition;speech analysis;strategic game planning;state-of-the-art models;transparency;Predictive models;Machine learning;Support vector machines;Decision trees;Supervised learning;Optimization;explainable artificial intelligence;interpretability;explainability;comprehensibility}, 
doi={10.23919/MIPRO.2018.8400040}, 
ISSN={}, 
month={May},}

@ARTICLE{Fuzzy19, 
author={A. {Fernandez} and F. {Herrera} and O. {Cordon} and M. {Jose del Jesus} and F. {Marcelloni}}, 
journal={IEEE Computational Intelligence Magazine}, 
title={Evolutionary Fuzzy Systems for Explainable Artificial Intelligence: Why, When, What for, and Where to?}, 
year={2019}, 
volume={14}, 
number={1}, 
pages={69-81}, 
keywords={data handling;evolutionary computation;expert systems;fuzzy set theory;fuzzy systems;learning (artificial intelligence);explainable Artificial Intelligence;evolutionary algorithms;fuzzy modeling;eXplainable Artificial Intelligence;evolutionary fuzzy systems;computational intelligence;data science;4W questions;Fuzzy systems;Data models;Data science;Fuzzy sets;Computational modeling;Task analysis;Genetic algorithms;Zadeh, Lotfi}, 
doi={10.1109/MCI.2018.2881645}, 
ISSN={1556-603X}, 
month={Feb},}

@ARTICLE{Hagras18, 
author={H. {Hagras}}, 
journal={Computer}, 
title={Toward Human-Understandable, Explainable AI}, 
year={2018}, 
volume={51}, 
number={9}, 
pages={28-36}, 
keywords={artificial intelligence;fuzzy logic;computing power;artificial intelligence;XAI;type-2 fuzzy logic systems;AI systems;human-understandable;Artificial intelligence;Machine learning;Learning systems;Fuzzy logic;Intelligent systems;Future of AI;artificial intelligence;AI;intelligent systems;explainable artificial intelligence;machine leaning;Type-2 Fuzzy Logic Systems}, 
doi={10.1109/MC.2018.3620965}, 
ISSN={0018-9162}, 
month={Sep.},}

@INPROCEEDINGS{GP18, 
author={D. {Howard} and M. A. {Edwards}}, 
booktitle={2018 International Conference on Machine Learning and Data Engineering (iCMLDE)}, 
title={Explainable A.I.: The Promise of Genetic Programming Multi-run Subtree Encapsulation}, 
year={2018}, 
volume={}, 
number={}, 
pages={158-159}, 
keywords={data encapsulation;genetic algorithms;learning (artificial intelligence);neural nets;trees (mathematics);white-box solutions;artificial neural network;explainable artificial intelligence;genetic programming;deep learning;multirun subtree encapsulation;explainable AI;Encapsulation;Databases;Genetic programming;Standards;Deep learning;Artificial neural networks;Explainable Artificial Intelligence;A.I.;Genetic Programming;Evolutionary Computation;modularization;Subtree Encapsulation;Automatically Defined Functions;Software Evolution;white box;black box;expression simplification;Deep Learning;Artificial Neural Networks;Multirun Subtree Encapsulation;subtree database}, 
doi={10.1109/iCMLDE.2018.00037}, 
ISSN={}, 
month={Dec},}

@article{LEE1994,
title = {Building Skeleton Models via 3-D Medial Surface Axis Thinning Algorithms},
journal = {CVGIP: Graphical Models and Image Processing},
volume = {56},
number = {6},
pages = {462-478},
year = {1994},
issn = {1049-9652},
doi = {https://doi.org/10.1006/cgip.1994.1042},
url = {https://www.sciencedirect.com/science/article/pii/S104996528471042X},
author = {T.C. Lee and R.L. Kashyap and C.N. Chu},
}

@ARTICLE{506792,
  author={Casey, R.G. and Lecolinet, E.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A survey of methods and strategies in character segmentation}, 
  year={1996},
  volume={18},
  number={7},
  pages={690-706},
  doi={10.1109/34.506792}}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}